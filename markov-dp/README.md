# Simulaci√≥n MDP para Navegaci√≥n de Robot - Proyecto Rob√≥tica INFO1167

## Descripci√≥n General

Este proyecto implementa un **Proceso de Decisi√≥n de Markov (MDP)** para resolver un problema de navegaci√≥n rob√≥tica en un entorno 2D. El robot debe navegar por un mapa de cuadr√≠cula de 6√ó8 para alcanzar un objetivo (meta) mientras maximiza sus recompensas y evita obst√°culos y zonas peligrosas.

### Problema Planteado

Seg√∫n la especificaci√≥n del proyecto, se debe implementar el algoritmo **MDP Q(s,a) - Value Iteration** para:

- Construir matrices de transici√≥n para cada acci√≥n (N, S, E, O)
- Utilizar 4 factores de descuento (Œª) ‚Üí (0.86, 0.90, 0.94, 0.98)
- Probar 4 pol√≠ticas √≥ptimas con variaci√≥n de probabilidad de √©xito ‚Üí (0.5, 0.7, 0.8, 0.9)
- Evaluar la robustez de las pol√≠ticas mediante simulaci√≥n gr√°fica durante 1000 pasos

## Arquitectura del Sistema

### 1. Estructura del Mapa (`map.rs`)

El mapa est√° definido como una cuadr√≠cula de 6√ó8 con diferentes tipos de estados:

```rust
#[derive(Debug, PartialEq, Eq, Clone)]
pub enum StatusType {
    Normal,   // Estados normales (S) - Recompensa: -0.1
    Danger,   // Estados peligrosos (P) - Recompensa: -0.5
    Wall,     // Muros/obst√°culos (O) - Recompensa: -0.1
    Goal,     // Meta (M) - Recompensa: +10.0
}
```

**Configuraci√≥n del mapa:**

```rust
let raw_map = vec![
    [ "S0",  "S1",  "P1",  "O1",  "S3",  "O2",  "S4",  "S5"  ],
    [ "O3",  "S6",  "S7",  "S8",  "S9",  "S10", "S11", "O4"  ],
    [ "S12", "P2",  "S14", "O5",  "S15", "P3",  "S17", "S18" ],
    [ "S19", "S20", "S21", "S22", "M",   "S24", "S25", "O6"  ],
    [ "S26", "O7",  "O8",  "S27", "S28", "S29", "P4",  "S31" ],
    [ "S32", "O9",  "S33", "S34", "O10", "S35", "S36", "S37" ],
];
```

### 2. Algoritmo MDP - Value Iteration (`mdp.rs`)

#### Construcci√≥n de Matrices de Transici√≥n

El sistema construye matrices de transici√≥n para cada acci√≥n considerando:

- **Probabilidad principal**: 80% de ejecutar la acci√≥n deseada
- **Probabilidades laterales**: 10% cada una para desviarse a izquierda/derecha

```rust
pub fn build_transition_matrix_static(map: &Map) -> Vec<Vec<Vec<f32>>> {
    // matrices[action][from][to]
    let mut matrices = vec![
        vec![vec![0.0; N_STATES]; N_STATES], // North
        vec![vec![0.0; N_STATES]; N_STATES], // South
        vec![vec![0.0; N_STATES]; N_STATES], // East
        vec![vec![0.0; N_STATES]; N_STATES], // West
    ];

    // Direcciones con probabilidades estoc√°sticas
    let directions = [
        [(-1, 0), (0, -1), (0, 1)], // North: principal, izq(W), der(E)
        [(1, 0), (0, 1), (0, -1)],  // South: principal, izq(E), der(W)
        [(0, 1), (-1, 0), (1, 0)],  // East: principal, izq(N), der(S)
        [(0, -1), (1, 0), (-1, 0)], // West: principal, izq(S), der(N)
    ];
    // ... construcci√≥n de matrices considerando muros y l√≠mites
}
```

#### Iteraci√≥n de Valores (Value Iteration)

La **Value Iteration** es el algoritmo central del MDP que nos permite encontrar la **pol√≠tica √≥ptima** de navegaci√≥n.

**¬øQu√© problema resuelve?**

El robot necesita decidir qu√© acci√≥n tomar en cada posici√≥n del mapa para llegar a la meta de la manera m√°s eficiente, considerando que:

- Sus movimientos no son 100% precisos (puede desviarse)
- Algunas zonas son m√°s peligrosas que otras
- Debe planificar a largo plazo, no solo el siguiente paso

**¬øC√≥mo funciona conceptualmente?**

El algoritmo simula mentalmente todas las posibles trayectorias que puede tomar el robot y calcula cu√°l es la mejor estrategia. Es como un jugador de ajedrez que piensa varios movimientos por adelantado, pero considerando probabilidades de √©xito/fallo.

**¬øQu√© hace el algoritmo paso a paso?**

```rust
pub fn value_iteration(&mut self, discount_factor: f32) {
    // Crear una tabla para almacenar qu√© tan buena es cada acci√≥n en cada posici√≥n
    let mut q = vec![vec![0.0_f32; 4]; N_STATES];

    let t = self.transition_matrix.clone(); // Probabilidades de movimiento

    // Repetir el c√°lculo muchas veces hasta que se estabilice
    for _ in 0..1000 {
        for s in 0..N_STATES { // Para cada posici√≥n del mapa (48 posiciones)
            for a in 0..4 { // Para cada direcci√≥n: Norte, Sur, Este, Oeste
                let mut valor_esperado = 0_f32;

                // Considerar todos los lugares donde podr√≠a terminar el robot
                for posicion_destino in 0..N_STATES {
                    // ¬øQu√© tan probable es llegar a esta posici√≥n?
                    let probabilidad = t[a][s][posicion_destino];

                    // ¬øQu√© recompensa obtendr√© al llegar ah√≠?
                    let recompensa_inmediata = self.map.states[
                        posicion_destino / N_COLS
                    ][posicion_destino % N_COLS].reward;

                    // ¬øCu√°l es el mejor valor que puedo obtener desde ah√≠?
                    let mejor_valor_futuro = q[posicion_destino].clone()
                        .into_iter().reduce(f32::max).unwrap_or(0.0);

                    // Combinar todo: Probabilidad √ó (Recompensa + Valor futuro)
                    valor_esperado += probabilidad * (
                        recompensa_inmediata + discount_factor * mejor_valor_futuro
                    );
                }

                // Guardar qu√© tan buena es esta acci√≥n en esta posici√≥n
                q[s][a] = valor_esperado;
            }
        }
    }

    self.q_values = q; // Guardar la tabla final
}
```

**¬øQu√© logra este proceso?**

1. **Tabla de decisiones**: Al final, tenemos una tabla que dice "en la posici√≥n X, la mejor acci√≥n es Y"

2. **Aprendizaje por simulaci√≥n**: El robot "practica" mentalmente miles de escenarios sin moverse f√≠sicamente

3. **Considera incertidumbre**: Toma en cuenta que puede fallar el 20% de las veces

4. **Planificaci√≥n inteligente**: No solo busca el camino m√°s corto, sino el m√°s seguro considerando recompensas

**¬øPor qu√© usamos factores de descuento?**

El `discount_factor` controla la "paciencia" del robot:

- **0.86**: "Prefiero llegar r√°pido, aunque el camino sea m√°s arriesgado"
- **0.90**: "Balance entre velocidad y seguridad"
- **0.94**: "Prefiero un camino m√°s seguro aunque tome m√°s tiempo"
- **0.98**: "Estoy dispuesto a tomar rutas muy largas si eso garantiza √©xito"

**¬øC√≥mo se obtiene la pol√≠tica final?**

```rust
pub fn get_max_policy(&mut self) -> Vec<usize> {
    let mut politica_optima = vec![0; N_STATES];

    for (posicion, valores_acciones) in self.q_values.iter().enumerate() {
        // Para cada posici√≥n, encontrar la mejor acci√≥n
        let mejor_valor = valores_acciones.iter().reduce(f32::max).unwrap();
        let mejor_accion = valores_acciones.iter()
            .position(|x| *x == *mejor_valor).unwrap();
        politica_optima[posicion] = mejor_accion;
    }

    politica_optima
}
```

**Resultado pr√°ctico:**

El algoritmo nos da una **estrategia completa** que dice exactamente qu√© hacer en cada situaci√≥n:

- "Si estoy en la posici√≥n (2,3), ir al Norte"
- "Si estoy en la posici√≥n (4,1), ir al Este"
- Y as√≠ para las 48 posiciones del mapa

Esta estrategia es **√≥ptima** porque maximiza las recompensas esperadas considerando tanto las recompensas inmediatas como las futuras.

### 3. Robot Aut√≥nomo (`robot.rs`)

El robot implementa un comportamiento estoc√°stico basado en las probabilidades de √©xito configuradas:

```rust
fn calc_next_action(&self, next_action: usize) -> (f32, f32) {
    let north = (0.0, -TILE_SIZE);
    let south = (0.0, TILE_SIZE);
    let east = (TILE_SIZE, 0.0);
    let west = (-TILE_SIZE, 0.0);

    let combinations = [
        [north, east, west],
        [south, east, west],
        [east, north, south],
        [west, south, north],
    ];

    let possible_actions = combinations[next_action];
    let success_prob = SUCCESS_PROBABILITIES[self.success_prob] as f32;

    let choice = rand::random::<f32>();

    if choice <= success_prob {
        possible_actions[0]
    } else {
        match rand::random_bool(0.5) {
            true => possible_actions[1],
            false => possible_actions[2],
        }
    }
```

### 4. Sistema de Simulaci√≥n (`core.rs`)

#### Simulaci√≥n Masiva de Datos

El n√∫cleo del sistema ejecuta simulaciones exhaustivas:

```rust
pub fn run_simulation() -> Vec<Vec<Vec<f32>>> {
    let mut results = vec![vec![vec![]; 4]; 4]; // [success_prob][discount_factor]

    // Para cada combinaci√≥n de par√°metros
    for success_prob in 0..4 {
        for discount_factor in 0..4 {
            let mut simulation_steps = 0;
            let mut rewards = vec![];

            while simulation_steps < 1000 {
                robot.update(&mdps[discount_factor].get_max_policy(), &map);
                simulation_steps += 1;

                let robot_pos = robot.get_matricial_position();
                rewards.push(map.states[robot_pos[0]][robot_pos[1]].reward);

                // Reiniciar robot al alcanzar la meta
                if robot.get_position() == map.get_goal_position() {
                    let new_position = map.get_random_valid_position();
                    robot.set_position(new_position);
                }
            }
            results[success_prob][discount_factor] = rewards;
        }
    }
    results
}
```

## Par√°metros de Configuraci√≥n

### Constantes del Sistema

```rust
pub const N_ROWS: usize = 6;
pub const N_COLS: usize = 8;
pub const N_STATES: usize = N_ROWS * N_COLS; // 48 estados
pub const PROBABILITIES: [f32; 3] = [0.8, 0.1, 0.1]; // Principal, Izq, Der

// Factores de descuento (Œª)
pub const DISCOUNT_FACTORS: [f32; 4] = [0.86, 0.90, 0.94, 0.98];

// Probabilidades de √©xito del robot
pub const SUCCESS_PROBABILITIES: [f32; 4] = [0.5, 0.7, 0.8, 0.9];
```

## Visualizaci√≥n y An√°lisis (`graphics.rs`)

El sistema genera gr√°ficos de recompensas acumulativas para analizar el desempe√±o:

```rust
pub fn graphic(results: &Vec<Vec<Vec<f32>>>) {
    // Crea 4 subgr√°ficos (uno por probabilidad de √©xito)
    // Cada gr√°fico muestra 4 curvas (una por factor de descuento)

    for (success_idx, success_prob_results) in results.iter().enumerate() {
        let mut cumulative_rewards = Vec::new();
        let mut cumulative_sum = 0.0;

        for &reward in rewards {
            cumulative_sum += reward;
            cumulative_rewards.push(cumulative_sum);
        }
        // ... renderizado con plotters
    }
}
```

## Funcionalidades Principales

### ‚úÖ Implementadas seg√∫n especificaci√≥n:

1. **Matrices de Transici√≥n**: Construidas para las 4 acciones (N,S,E,O) considerando probabilidades estoc√°sticas
2. **4 Factores de Descuento**: Œª = [0.86, 0.90, 0.94, 0.98]
3. **4 Probabilidades de √âxito**: [50%, 70%, 80%, 90%]
4. **Pol√≠ticas √ìptimas**: Calculadas mediante Value Iteration para cada configuraci√≥n
5. **Simulaci√≥n de 1000 pasos**: Evaluaci√≥n robusta de cada pol√≠tica
6. **Visualizaci√≥n gr√°fica**: Generaci√≥n autom√°tica de gr√°ficos de recompensas acumulativas

### üéÆ Funcionalidades adicionales:

- **Visualizaci√≥n interactiva en tiempo real** usando Raylib
- **Reinicio autom√°tico** del robot al alcanzar la meta
- **Validaci√≥n de movimientos** (l√≠mites del mapa y obst√°culos)
- **Posicionamiento aleatorio** inicial del robot

## Resultados y An√°lisis

Los resultados de la simulaci√≥n se visualizan en `rewards.png`, mostrando:

- **4 gr√°ficos** (uno por probabilidad de √©xito del robot)
- **4 curvas por gr√°fico** (una por factor de descuento)
- **Recompensas acumulativas** durante 1000 pasos de simulaci√≥n

Esto permite evaluar c√≥mo diferentes factores de descuento y probabilidades de √©xito afectan el desempe√±o del robot en el entorno, cumpliendo con los objetivos del m√≥dulo de MDP en rob√≥tica.

```

```
